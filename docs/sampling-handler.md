# Sampling Handler avec Confirmation Utilisateur

## Vue d'ensemble

Le client MCP peut impl√©menter un **sampling handler** personnalis√© pour contr√¥ler les demandes d'√©chantillonnage LLM du serveur. Notre handler demande une confirmation √† l'utilisateur et, si accept√©, appelle le LLM pour obtenir la r√©ponse JSON.

## Pourquoi un Sampling Handler ?

Quand le serveur MCP utilise `ctx.sample()` pour faire des analyses IA (comme le matching CFP ‚Üî Conf√©rences), il demande au client de fournir une r√©ponse LLM. Notre handler personnalis√© permet de :

- ‚úÖ **Contr√¥ler** les appels LLM (autoriser/refuser)
- ‚úÖ **Visualiser** ce que le serveur demande √† l'IA
- ‚úÖ **D√©boguer** les prompts de sampling
- ‚úÖ **Appeler le LLM** uniquement apr√®s confirmation

## Impl√©mentation

### Handler de Confirmation + Appel LLM

```python
from fastmcp.client.sampling import RequestContext, SamplingMessage, SamplingParams
import httpx

async def user_confirmation_sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext,
) -> str:
    """
    Sampling handler qui demande une confirmation y/n √† l'utilisateur.
    Si accept√©, appelle le LLM et retourne sa r√©ponse JSON.
    """
    # Afficher les informations sur la demande de sampling
    print("ü§ñ Le serveur MCP demande un √©chantillonnage LLM")

    # Afficher le system prompt
    if params.systemPrompt:
        print(f"üìã System Prompt: {params.systemPrompt[:500]}...")

    # Afficher les messages
    for message in messages:
        print(f"{message.role}: {message.content.text[:300]}...")

    # Afficher les param√®tres
    print(f"‚öôÔ∏è  Temperature: {params.temperature}")
    print(f"‚öôÔ∏è  Max tokens: {params.maxTokens}")

    # Demander confirmation
    response = input("‚ùì Autoriser cet √©chantillonnage LLM ? (y/n): ")

    if response.lower() in ["y", "yes", "o", "oui"]:
        print("‚úÖ √âchantillonnage autoris√©")
        print("üîÑ Appel du LLM...\n")

        # Construire les messages pour l'API OpenAI
        llm_messages = []

        # Ajouter le system prompt si pr√©sent
        if params.systemPrompt:
            llm_messages.append({"role": "system", "content": params.systemPrompt})

        # Ajouter les messages du sampling
        for message in messages:
            content_text = message.content.text if hasattr(message.content, "text") else str(message.content)
            llm_messages.append({"role": message.role, "content": content_text})

        # Appeler le LLM
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "http://localhost:4141/v1/chat/completions",
                json={
                    "model": "gpt-4o-mini",
                    "messages": llm_messages,
                    "temperature": params.temperature if params.temperature else 0.3,
                    "max_tokens": params.maxTokens if params.maxTokens else 4000,
                },
            )

            result = response.json()
            llm_response = result["choices"][0]["message"]["content"]

            print(f"‚úÖ R√©ponse du LLM re√ßue ({len(llm_response)} caract√®res)")
            return llm_response
    else:
        print("‚ùå √âchantillonnage refus√©")
        return '{"error": "Sampling denied by user"}'
```

### Enregistrement du Handler

Pour utiliser ce handler, passez-le au constructeur du client :

```python
from fastmcp.client import Client

client = Client(
    "http://127.0.0.1:8000/mcp",
    sampling_handler=user_confirmation_sampling_handler
)

async with client:
    # Les appels √† ctx.sample() du serveur utiliseront ce handler
    result = await client.call_tool(
        "search_conferences",
        {
            "cfp_open": True,
            "country": "France",
            "match_cfps": True,  # D√©clenchera des sampling
        }
    )
```

## Test

### Fichier de test : `test_sampling_with_handler.py`

```bash
# Terminal 1 : D√©marrer le serveur
uv run python mcp_server/server.py

# Terminal 2 : Tester avec le handler
uv run python test_sampling_with_handler.py
```

### Workflow

1. Le client appelle `search_conferences` avec `match_cfps=True`
2. Le serveur lit les CFPs et pour chaque CFP appelle `ctx.sample()`
3. Le client re√ßoit la demande de sampling via le handler
4. **L'utilisateur voit** :
   - Le system prompt du sampling
   - Les messages envoy√©s au LLM
   - Les param√®tres (temperature, max_tokens)
5. **L'utilisateur d√©cide** :
   - `y` ‚Üí Le handler appelle le LLM et retourne sa r√©ponse JSON
   - `n` ‚Üí Le handler retourne une erreur JSON
6. Le serveur re√ßoit la r√©ponse et continue le traitement

## Exemple de Sortie

```
================================================================================
ü§ñ Le serveur MCP demande un √©chantillonnage LLM
================================================================================

üìã System Prompt:
--------------------------------------------------------------------------------
Analyze which conferences match this CFP topic: "Model Context Protocol"...
--------------------------------------------------------------------------------

üí¨ Messages:
--------------------------------------------------------------------------------
üë§ Message 1 (user):
CFP excerpt:
# Model Context Protocol et agents IA

Ce talk pr√©sente le Model Context Protocol (MCP)...

Available conferences:
[
  {"name": "Devoxx France 2026", "tags": ["java", "development"]},
  ...
]
--------------------------------------------------------------------------------

‚öôÔ∏è  Param√®tres de sampling:
   - Temperature: 0.3
   - Max tokens: 4000

================================================================================
‚ùì Voulez-vous autoriser cet √©chantillonnage LLM ? (y/n): y
‚úÖ √âchantillonnage autoris√©
üîÑ Appel du LLM...

‚úÖ R√©ponse du LLM re√ßue (542 caract√®res)
```

Le serveur re√ßoit alors la r√©ponse JSON du LLM et peut continuer le matching.

## Structure du Sampling Handler

### Signature

```python
async def handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext,
) -> str:
```

### Param√®tres

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `messages` | `list[SamplingMessage]` | Liste des messages (role + content) |
| `params` | `SamplingParams` | Param√®tres du sampling (temperature, maxTokens, systemPrompt...) |
| `context` | `RequestContext` | Contexte de la requ√™te MCP |

### Retour

Le handler doit retourner une `str` qui sera utilis√©e comme r√©ponse du LLM par le serveur. Dans notre cas, c'est la r√©ponse JSON du LLM.

### Messages

Chaque `SamplingMessage` contient :
- `role` : `"user"` ou `"assistant"`
- `content` : Objet avec attribut `text` pour le contenu textuel

### Params

`SamplingParams` contient :
- `systemPrompt` : Prompt syst√®me optionnel
- `temperature` : Temp√©rature de sampling (0-1)
- `maxTokens` : Nombre maximum de tokens
- `modelPreferences` : Pr√©f√©rences de mod√®le
- `stopSequences` : S√©quences d'arr√™t
- `tools` : Liste des outils disponibles
- `toolChoice` : Choix d'utilisation des outils

## Handlers Pr√©d√©finis

FastMCP fournit des handlers pour les APIs courantes :

### OpenAI

```python
from fastmcp.client.openai import OpenAISamplingHandler

handler = OpenAISamplingHandler(default_model="gpt-4o")
client = Client(url, sampling_handler=handler)
```

Installation : `pip install fastmcp[openai]`

### Anthropic

```python
from fastmcp.client.anthropic import AnthropicSamplingHandler

handler = AnthropicSamplingHandler(default_model="claude-sonnet-4-5")
client = Client(url, sampling_handler=handler)
```

Installation : `pip install fastmcp[anthropic]`

## Cas d'Usage

### 1. D√©veloppement et D√©bogage

Utiliser le handler de confirmation pour :
- Voir exactement ce que le serveur envoie au LLM
- Comprendre les prompts de sampling
- Contr√¥ler chaque appel LLM

### 2. Contr√¥le des Co√ªts

Autoriser manuellement chaque appel LLM pour √©viter :
- Les boucles infinies
- Les appels non souhait√©s
- Les co√ªts inattendus

### 3. Tests Automatis√©s

Cr√©er un handler qui :
- Retourne des r√©ponses pr√©d√©finies
- Simule diff√©rents sc√©narios
- Teste sans vraie API LLM

```python
async def mock_sampling_handler(messages, params, context):
    # Retourner une r√©ponse mock√©e
    return '{"matches": [{"conference_name": "Test Conf", "score": 100}]}'
```

### 4. Production avec Validation

En production, ajouter une validation avant d'appeler le vrai LLM :
```python
async def validated_sampling_handler(messages, params, context):
    # V√©rifier que la demande est l√©gitime
    if not is_valid_request(messages, params):
        return '{"error": "Invalid request"}'

    # Appeler le vrai LLM
    return await call_real_llm(messages, params)
```

## Configuration

### Client avec Handler de Confirmation (D√©veloppement)

```python
client = Client(url, sampling_handler=user_confirmation_sampling_handler)
```

### Client avec LLM Direct (Production)

```python
from fastmcp.client.openai import OpenAISamplingHandler

handler = OpenAISamplingHandler(
    default_model="gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY")
)

client = Client(url, sampling_handler=handler)
```

### Sans Handler

Si aucun handler n'est fourni, le client ne supportera pas le sampling et les appels √† `ctx.sample()` √©choueront.

## Documentation FastMCP

Pour plus d'informations :
- [FastMCP Sampling Clients](https://gofastmcp.com/clients/sampling)
- [FastMCP Client API](https://gofastmcp.com/clients/python)

### Enregistrement du Handler

Pour utiliser ce handler, passez-le au constructeur du client :

```python
from fastmcp.client import Client

client = Client(
    "http://127.0.0.1:8000/mcp",
    sampling_handler=user_confirmation_sampling_handler
)

async with client:
    # Les appels √† ctx.sample() du serveur utiliseront ce handler
    result = await client.call_tool(
        "search_conferences",
        {
            "cfp_open": True,
            "country": "France",
            "match_cfps": True,  # D√©clenchera des sampling
        }
    )
```

## Test

### Fichier de test : `test_sampling_with_handler.py`

```bash
# Terminal 1 : D√©marrer le serveur
uv run python mcp_server/server.py

# Terminal 2 : Tester avec le handler
uv run python test_sampling_with_handler.py
```

### Workflow

1. Le client appelle `search_conferences` avec `match_cfps=True`
2. Le serveur lit les CFPs et pour chaque CFP appelle `ctx.sample()`
3. Le client re√ßoit la demande de sampling via le handler
4. **L'utilisateur voit** :
   - Le system prompt du sampling
   - Les messages envoy√©s au LLM
   - Les param√®tres (temperature, max_tokens)
5. **L'utilisateur d√©cide** :
   - `y` ‚Üí Le handler retourne une r√©ponse positive
   - `n` ‚Üí Le handler retourne une r√©ponse n√©gative
6. Le serveur continue avec la r√©ponse du handler

## Exemple de Sortie

```
================================================================================
ü§ñ Le serveur MCP demande un √©chantillonnage LLM
================================================================================

üìã System Prompt:
--------------------------------------------------------------------------------
Analyze which conferences match this CFP topic: "Model Context Protocol"...
--------------------------------------------------------------------------------

üí¨ Messages:
--------------------------------------------------------------------------------
üë§ Message 1 (user):
CFP excerpt:
# Model Context Protocol et agents IA

Ce talk pr√©sente le Model Context Protocol (MCP)...

Available conferences:
[
  {"name": "Devoxx France 2026", "tags": ["java", "development"]},
  ...
]
--------------------------------------------------------------------------------

‚öôÔ∏è  Param√®tres de sampling:
   - Temperature: 0.3
   - Max tokens: 4000

================================================================================
‚ùì Voulez-vous autoriser cet √©chantillonnage LLM ? (y/n): y
‚úÖ √âchantillonnage autoris√©
```

## Structure du Sampling Handler

### Signature

```python
async def handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext,
) -> str:
```

### Param√®tres

| Param√®tre | Type | Description |
|-----------|------|-------------|
| `messages` | `list[SamplingMessage]` | Liste des messages (role + content) |
| `params` | `SamplingParams` | Param√®tres du sampling (temperature, maxTokens, systemPrompt...) |
| `context` | `RequestContext` | Contexte de la requ√™te MCP |

### Retour

Le handler doit retourner une `str` qui sera utilis√©e comme r√©ponse du LLM par le serveur.

### Messages

Chaque `SamplingMessage` contient :
- `role` : `"user"` ou `"assistant"`
- `content` : Objet avec attribut `text` pour le contenu textuel

### Params

`SamplingParams` contient :
- `systemPrompt` : Prompt syst√®me optionnel
- `temperature` : Temp√©rature de sampling (0-1)
- `maxTokens` : Nombre maximum de tokens
- `modelPreferences` : Pr√©f√©rences de mod√®le
- `stopSequences` : S√©quences d'arr√™t
- `tools` : Liste des outils disponibles
- `toolChoice` : Choix d'utilisation des outils

## Handlers Pr√©d√©finis

FastMCP fournit des handlers pour les APIs courantes :

### OpenAI

```python
from fastmcp.client.openai import OpenAISamplingHandler

handler = OpenAISamplingHandler(default_model="gpt-4o")
client = Client(url, sampling_handler=handler)
```

Installation : `pip install fastmcp[openai]`

### Anthropic

```python
from fastmcp.client.anthropic import AnthropicSamplingHandler

handler = AnthropicSamplingHandler(default_model="claude-sonnet-4-5")
client = Client(url, sampling_handler=handler)
```

Installation : `pip install fastmcp[anthropic]`

## Cas d'Usage

### 1. D√©veloppement et D√©bogage

Utiliser le handler de confirmation pour :
- Voir exactement ce que le serveur envoie au LLM
- Comprendre les prompts de sampling
- Tester sans consommer de cr√©dits API

### 2. Contr√¥le des Co√ªts

Autoriser manuellement chaque appel LLM pour √©viter :
- Les boucles infinies
- Les appels non souhait√©s
- Les co√ªts inattendus

### 3. Tests Automatis√©s

Cr√©er un handler qui :
- Retourne des r√©ponses pr√©d√©finies
- Simule diff√©rents sc√©narios
- Teste sans vraie API LLM

```python
async def mock_sampling_handler(messages, params, context):
    # Retourner une r√©ponse mock√©e
    return '{"matches": [{"conference_name": "Test Conf", "score": 100}]}'
```

### 4. Production avec Validation

En production, ajouter une validation avant d'appeler le vrai LLM :
```python
async def validated_sampling_handler(messages, params, context):
    # V√©rifier que la demande est l√©gitime
    if not is_valid_request(messages, params):
        return "Invalid request"

    # Appeler le vrai LLM
    return await call_real_llm(messages, params)
```

## Configuration

### Client Simple (Test)

```python
client = Client(url, sampling_handler=user_confirmation_sampling_handler)
```

### Client avec LLM (Production)

```python
from fastmcp.client.openai import OpenAISamplingHandler

handler = OpenAISamplingHandler(
    default_model="gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY")
)

client = Client(url, sampling_handler=handler)
```

### Sans Handler

Si aucun handler n'est fourni, le client ne supportera pas le sampling et les appels √† `ctx.sample()` √©choueront.

## Documentation FastMCP

Pour plus d'informations :
- [FastMCP Sampling Clients](https://gofastmcp.com/clients/sampling)
- [FastMCP Client API](https://gofastmcp.com/clients/python)
