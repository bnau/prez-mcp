"""MCP Client intelligent utilisant le pattern Prompt -> LLM -> Tool Call."""

import asyncio
import json

import httpx
from fastmcp.client import Client
from fastmcp.client.elicitation import ElicitResult
from fastmcp.client.sampling import RequestContext, SamplingMessage, SamplingParams


async def user_confirmation_sampling_handler(
    messages: list[SamplingMessage],
    params: SamplingParams,
    context: RequestContext,
) -> str:
    """
    Sampling handler qui demande une confirmation y/n √† l'utilisateur.

    Ce handler est appel√© par le serveur MCP quand il a besoin d'un √©chantillonnage LLM.
    Si l'utilisateur accepte, on appelle le LLM et on retourne sa r√©ponse JSON.
    """
    print("\n" + "=" * 80)
    print("ü§ñ Le serveur MCP demande un √©chantillonnage LLM")
    print("=" * 80)

    # Afficher le system prompt si pr√©sent
    if params.systemPrompt:
        print("\nüìã System Prompt:")
        print("-" * 80)
        print(params.systemPrompt[:500])
        if len(params.systemPrompt) > 500:
            print("... (tronqu√©)")
        print("-" * 80)

    # Afficher les messages
    print("\nüí¨ Messages:")
    print("-" * 80)
    for i, message in enumerate(messages, 1):
        role_emoji = "üë§" if message.role == "user" else "ü§ñ"
        print(f"{role_emoji} Message {i} ({message.role}):")

        # Extraire le contenu
        content_text = ""
        if hasattr(message.content, "text"):
            content_text = message.content.text
        else:
            content_text = str(message.content)

        # Afficher un extrait
        if len(content_text) > 300:
            print(content_text[:300] + "... (tronqu√©)")
        else:
            print(content_text)
        print()

    print("-" * 80)

    # Afficher les param√®tres de sampling
    print("\n‚öôÔ∏è  Param√®tres de sampling:")
    if params.temperature is not None:
        print(f"   - Temperature: {params.temperature}")
    if params.maxTokens is not None:
        print(f"   - Max tokens: {params.maxTokens}")
    if params.modelPreferences:
        print(f"   - Model preferences: {params.modelPreferences}")

    # Demander confirmation √† l'utilisateur
    print("\n" + "=" * 80)
    while True:
        prompt_msg = "‚ùì Voulez-vous autoriser cet √©chantillonnage LLM ? (y/n): "
        response = input(prompt_msg).strip().lower()
        if response in ["y", "yes", "o", "oui"]:
            print("‚úÖ √âchantillonnage autoris√©")
            print("üîÑ Appel du LLM...\n")

            # Construire les messages pour l'API OpenAI
            llm_messages = []

            # Ajouter le system prompt si pr√©sent
            if params.systemPrompt:
                llm_messages.append({"role": "system", "content": params.systemPrompt})

            # Ajouter les messages du sampling
            for message in messages:
                content_text = ""
                if hasattr(message.content, "text"):
                    content_text = message.content.text
                else:
                    content_text = str(message.content)

                llm_messages.append({"role": message.role, "content": content_text})

            # Appeler le LLM
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.post(
                        "http://localhost:4141/v1/chat/completions",
                        json={
                            "model": "gpt-4o-mini",
                            "messages": llm_messages,
                            "temperature": params.temperature if params.temperature else 0.3,
                            "max_tokens": params.maxTokens if params.maxTokens else 4000,
                        },
                    )

                    if response.status_code != 200:
                        error_msg = f"Erreur API LLM: {response.status_code}"
                        print(f"‚ùå {error_msg}")
                        return f'{{"error": "{error_msg}"}}'

                    result = response.json()
                    llm_response = result["choices"][0]["message"]["content"]

                    print(f"‚úÖ R√©ponse du LLM re√ßue ({len(llm_response)} caract√®res)")
                    return llm_response

            except Exception as e:
                error_msg = f"Erreur lors de l'appel LLM: {str(e)}"
                print(f"‚ùå {error_msg}")
                return f'{{"error": "{error_msg}"}}'

        elif response in ["n", "no", "non"]:
            print("‚ùå √âchantillonnage refus√©\n")
            # Retourner une r√©ponse n√©gative
            return '{"error": "Sampling denied by user"}'
        else:
            print("‚ö†Ô∏è  R√©ponse invalide. Utilisez 'y' ou 'n'.")


async def elicitation_handler(prompt: str, response_type: type | None, params, context):
    """
    Handler d'√©licitation qui demande une r√©ponse y/n √† l'utilisateur.

    Ce handler est appel√© par le serveur MCP quand il a besoin d'une confirmation
    ou d'une information de la part de l'utilisateur.

    Args:
        prompt: Le texte de la question pos√©e par le serveur
        response_type: Le type de r√©ponse attendu (bool, str, int, None, ou dataclass)
        params: Param√®tres d'√©licitation
        context: Contexte de la requ√™te
    """
    print("\n" + "=" * 80)
    print("‚ùì Le serveur demande une confirmation")
    print("=" * 80)
    print(f"\n{prompt}\n")

    # Boucle pour obtenir une r√©ponse valide
    while True:
        answer = input("R√©pondez (y/n ou 'cancel' pour annuler): ").strip().lower()

        # Annuler l'op√©ration
        if answer == "cancel":
            print("‚ö†Ô∏è  Op√©ration annul√©e\n")
            return ElicitResult(action="cancel")

        # D√©cliner de r√©pondre
        if answer == "decline":
            print("‚ÑπÔ∏è  Vous avez d√©clin√© de r√©pondre\n")
            return ElicitResult(action="decline")

        # R√©ponse oui/non pour type bool
        if answer in ["y", "yes", "o", "oui"]:
                print("‚úÖ R√©ponse: Oui\n")
                return ElicitResult(action="accept", content=True)

        if answer in ["n", "no", "non"]:
                print("‚ùå R√©ponse: Non\n")
                return ElicitResult(action="accept", content=False)

        # Pour d'autres types, accepter directement la r√©ponse
        elif response_type is str:
            print(f"‚úÖ R√©ponse: {answer}\n")
            return ElicitResult(action="accept", content=answer)

        # R√©ponse invalide
        print("‚ö†Ô∏è  R√©ponse invalide. Utilisez 'y' pour oui, 'n' pour non.")


class LLMOrchestrator:
    """Orchestrateur qui utilise un LLM pour d√©cider quels outils MCP appeler."""

    def __init__(self, base_url: str = "http://localhost:4141"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=60.0)

    async def close(self):
        """Ferme le client HTTP."""
        await self.client.aclose()

    async def execute_prompt_with_tools(
        self, prompt_text: str, available_tools: list[dict], mcp_client: Client
    ) -> dict:
        """
        Envoie un prompt au LLM avec la liste des outils disponibles.
        Le LLM d√©cide quels outils appeler et avec quels param√®tres.

        Args:
            prompt_text: Le texte du prompt √† ex√©cuter
            available_tools: Liste des outils MCP disponibles
            mcp_client: Client MCP pour appeler les outils

        Returns:
            Dictionnaire avec les r√©sultats
        """
        # Formater les outils pour l'API OpenAI
        tools_spec = []
        for tool in available_tools:
            tool_spec = {
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": tool["inputSchema"],
                },
            }
            tools_spec.append(tool_spec)

        # Premi√®re requ√™te : le LLM d√©cide quels outils appeler
        messages = [{"role": "user", "content": prompt_text}]

        print("ü§ñ Envoi du prompt au LLM pour qu'il d√©cide des outils √† appeler...")

        response = await self.client.post(
            f"{self.base_url}/v1/chat/completions",
            json={
                "model": "gpt-4o-mini",
                "messages": messages,
                "tools": tools_spec,
                "temperature": 0.3,
            },
        )

        if response.status_code != 200:
            raise Exception(f"Erreur API LLM: {response.status_code} - {response.text}")

        result = response.json()
        assistant_message = result["choices"][0]["message"]

        # Si le LLM veut appeler des outils
        if "tool_calls" in assistant_message:
            print(f"üîß Le LLM a d√©cid√© d'appeler {len(assistant_message['tool_calls'])} outil(s)\n")

            # Ajouter le message de l'assistant √† l'historique
            messages.append(assistant_message)

            # Ex√©cuter chaque tool call
            tool_results = {}
            for tool_call in assistant_message["tool_calls"]:
                function_name = tool_call["function"]["name"]
                function_args = json.loads(tool_call["function"]["arguments"])

                print(f"   üìû Appel de l'outil: {function_name}")
                print(f"   üìã Param√®tres: {json.dumps(function_args, indent=6)}")

                # Appeler l'outil MCP
                try:
                    tool_result = await mcp_client.call_tool(function_name, function_args)

                    # Extraire le contenu textuel
                    result_content = ""
                    for content in tool_result.content:
                        if hasattr(content, "text"):
                            result_content += content.text

                    print(f"   ‚úÖ R√©sultat re√ßu ({len(result_content)} caract√®res)\n")

                    # Stocker le r√©sultat brut pour utilisation ult√©rieure
                    tool_results[function_name] = result_content

                    # Pour le LLM, envoyer juste un r√©sum√© si trop long
                    llm_content = result_content
                    if len(result_content) > 10000:
                        # Tronquer pour le LLM
                        llm_content = (
                            "R√©sultat trop volumineux pour √™tre inclus enti√®rement. "
                            f"Taille: {len(result_content)} caract√®res. "
                            "Les donn√©es ont √©t√© stock√©es et seront trait√©es "
                            "directement par le client."
                        )
                        print(
                            "   ‚ö†Ô∏è  R√©sultat trop volumineux, envoi d'un r√©sum√© "
                            "au LLM au lieu du JSON complet\n"
                        )

                    # Ajouter le r√©sultat √† l'historique
                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call["id"],
                            "name": function_name,
                            "content": llm_content,
                        }
                    )
                except Exception as e:
                    print(f"   ‚ùå Erreur lors de l'appel: {e}\n")
                    messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call["id"],
                            "name": function_name,
                            "content": f"Erreur: {str(e)}",
                        }
                    )

            # Deuxi√®me requ√™te : le LLM traite les r√©sultats des outils
            print("ü§ñ Le LLM analyse les r√©sultats des outils...")
            response = await self.client.post(
                f"{self.base_url}/v1/chat/completions",
                json={"model": "gpt-4o-mini", "messages": messages, "temperature": 0.3},
            )

            if response.status_code != 200:
                error_detail = ""
                try:
                    error_detail = response.text
                except Exception:
                    pass
                raise Exception(
                    f"Erreur API LLM (2√®me requ√™te): {response.status_code}\n{error_detail[:500]}"
                )

            result = response.json()
            final_response = result["choices"][0]["message"]["content"]

            return {
                "type": "tool_result",
                "content": final_response,
                "messages": messages,
                "raw_tool_results": tool_results,
            }

        else:
            # Le LLM a r√©pondu directement sans appeler d'outils
            return {
                "type": "direct_response",
                "content": assistant_message.get("content", ""),
                "messages": messages,
            }


async def run_intelligent_client():
    """Client qui utilise le pattern Prompt -> LLM -> Tool Call."""
    mcp_url = "http://127.0.0.1:8000/mcp"
    mcp_client = Client(
        mcp_url,
        sampling_handler=user_confirmation_sampling_handler,
        elicitation_handler=elicitation_handler,
    )
    orchestrator = LLMOrchestrator()

    try:
        async with mcp_client:
            print("=" * 100)
            print("üéØ CLIENT MCP INTELLIGENT - Pattern Prompt ‚Üí LLM ‚Üí Tool Call")
            print("=" * 100)
            print()

            # √âtape 1 : R√©cup√©rer le prompt du serveur MCP
            print("=" * 100)
            print("üìã √âTAPE 1 : R√©cup√©ration du prompt depuis le serveur MCP")
            print("=" * 100)
            print()

            # Demander le prompt pour les conf√©rences en France
            prompt_result = await mcp_client.get_prompt(
                "find_conferences_for_open_cfps", {"country": "France"}
            )
            prompt_text = ""
            if hasattr(prompt_result, "messages"):
                for message in prompt_result.messages:
                    if hasattr(message.content, "text"):
                        prompt_text = message.content.text
                    else:
                        prompt_text = str(message.content)

            print("üìù Prompt re√ßu du serveur:")
            print("-" * 100)
            print(prompt_text)
            print("-" * 100)
            print()

            # √âtape 2 : R√©cup√©rer la liste des outils disponibles
            print("=" * 100)
            print("üîß √âTAPE 2 : R√©cup√©ration des outils MCP disponibles")
            print("=" * 100)
            print()

            tools_list = await mcp_client.list_tools()
            tools_for_llm = []
            for tool in tools_list:
                tools_for_llm.append(
                    {
                        "name": tool.name,
                        "description": tool.description,
                        "inputSchema": tool.inputSchema,
                    }
                )
                print(f"   üîß {tool.name}: {tool.description[:80]}...")

            print()

            # √âtape 3 : Envoyer le prompt au LLM avec les outils
            print("=" * 100)
            print("ü§ñ √âTAPE 3 : Le LLM analyse le prompt et d√©cide des actions")
            print("=" * 100)
            print()

            llm_result = await orchestrator.execute_prompt_with_tools(
                prompt_text, tools_for_llm, mcp_client
            )

            print("\nüí¨ R√©ponse finale du LLM:")
            print("=" * 100)
            content = llm_result["content"]
            print(content)
            print("=" * 100)
            print()

    finally:
        await orchestrator.close()


async def main():
    """Point d'entr√©e principal."""
    try:
        await run_intelligent_client()
    except ConnectionError as e:
        print("‚ùå Erreur de connexion")
        print("   Serveur MCP: http://127.0.0.1:8000/mcp")
        print("   API d'inf√©rence: http://localhost:4141")
        print(f"   D√©tails: {e}")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
